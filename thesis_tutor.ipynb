{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diangelion/thesis-tutor/blob/main/thesis_tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzIUIN9xwOd3"
      },
      "source": [
        "# THESIS TUTOR AI AGENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQt-F3MrEk_q"
      },
      "source": [
        "## Instalation and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0kxHpGBEjGg"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain==0.3.24 \\\n",
        "                 langchain-core==0.3.55 \\\n",
        "                 langchain-community==0.3.22 \\\n",
        "                 langchain-huggingface==0.1.2 \\\n",
        "                 langchain-chroma==0.2.3 \\\n",
        "                 hf_xet pymupdf pymupdf4llm \\\n",
        "                 firebase-admin sympy pylatexenc \\\n",
        "                 langchain-experimental langgraph \\\n",
        "                 langgraph-checkpoint-sqlite \\\n",
        "                 langsmith transformers tavily-python \\\n",
        "                 presidio-analyzer presidio-anonymizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxHkMMtlXDMp"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Libraries\n",
        "#=======================\n",
        "import os\n",
        "import shutil\n",
        "import pymupdf\n",
        "import firebase_admin\n",
        "\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from pymupdf4llm import to_markdown\n",
        "from firebase_admin import credentials, firestore\n",
        "from sympy import sympify, latex\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.schema import Document\n",
        "from langchain.agents import AgentExecutor, create_react_agent, AgentOutputParser\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.runnables import (\n",
        "    ConfigurableFieldSpec,\n",
        "    RunnablePassthrough,\n",
        "    RunnableLambda\n",
        ")\n",
        "\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_huggingface import (\n",
        "    HuggingFaceEmbeddings,\n",
        "    HuggingFaceEndpoint,\n",
        "    ChatHuggingFace\n",
        ")\n",
        "\n",
        "from langchain_core.agents import AgentFinish\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import (\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    PromptTemplate\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg27u01b2SGR"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J10_FECG2Tcy"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Configuration\n",
        "#=======================\n",
        "CONFIG = {\n",
        "    # User\n",
        "    \"user_session_id\": \"testing\",\n",
        "    # Misc\n",
        "    \"base_source_dir\": \"/content/drive/MyDrive/Skripsi/\",\n",
        "    \"persist_chroma_db\": \"/content/drive/MyDrive/LLM/thesis-tutor/chroma_db\",\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 400,\n",
        "    \"RESET_DB\": False,\n",
        "    # Firebase\n",
        "    \"firebase_key_file\": \"/content/drive/MyDrive/LLM/thesis-tutor/firebase-admin.json\",\n",
        "    \"firebase_collection\": \"chat_history\",\n",
        "    \"firebase_user_id\": \"id-1\",\n",
        "    # LLM\n",
        "    \"llm_repo_id\": \"Qwen/Qwen3-1.7B\",\n",
        "    # Embedding\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"embedding_model_kwargs\": {\"device\": \"cpu\"},\n",
        "    \"embedding_encode_kwargs\": {\"normalize_embeddings\": False},\n",
        "    # Safety\n",
        "    \"safety_model_classifier\": \"protectai/deberta-v3-base-prompt-injection-v2\",\n",
        "    \"safety_max_history_length\": 10,\n",
        "    \"safety_pii_entities\": [\"US_SSN\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\"],\n",
        "    \"safety_risk_keywords\": [\"password\", \"credit card\", \"social security\"],\n",
        "    \"safety_threshold\": 0.85\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0GUfMSv_JIx"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDzZIN0B_ETI"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# ChromaDB Checking\n",
        "#=======================\n",
        "def chroma_db_exists(persist_dir: str):\n",
        "    return os.path.exists(persist_dir) and os.path.isdir(persist_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta0OzXRArC7Y"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Source Gathering\n",
        "#=======================\n",
        "def get_doc_sources() -> list:\n",
        "  files_path = []\n",
        "\n",
        "  # Papers\n",
        "  papers_dir = CONFIG[\"base_source_dir\"] + \"Papers/\"\n",
        "  for file_name in os.listdir(papers_dir):\n",
        "    if os.path.isfile(os.path.join(papers_dir, file_name)):\n",
        "      files_path.append(papers_dir + file_name)\n",
        "\n",
        "  # Books\n",
        "  books_dir = CONFIG[\"base_source_dir\"] + \"Books/\"\n",
        "  for file_name in os.listdir(books_dir):\n",
        "    if os.path.isfile(os.path.join(books_dir, file_name)):\n",
        "      files_path.append(books_dir + file_name)\n",
        "\n",
        "  return files_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy7H3mYlsOez"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Document Loading\n",
        "#=======================\n",
        "def get_doc_loaded(doc_sources: list = []) -> list:\n",
        "  all_docs = []\n",
        "\n",
        "  if len(doc_sources) > 0:\n",
        "    # Process all PDFs\n",
        "    for file_path in tqdm(doc_sources, desc=\"Processing PDFs\"):\n",
        "      path = Path(file_path)\n",
        "      with pymupdf.open(path) as doc:\n",
        "        # Extract full text as Markdown (preserves math/tables across pages)\n",
        "        # Process PER PAGE for metadata\n",
        "        page_md = to_markdown(doc, page_chunks=True, show_progress=False)\n",
        "        for page in page_md:\n",
        "          all_docs.append(Document(\n",
        "              page_content=page[\"text\"],\n",
        "              metadata={\n",
        "                  \"title\": path.name,\n",
        "                  \"page\": page[\"metadata\"][\"page\"],\n",
        "              }\n",
        "          ))\n",
        "\n",
        "    # Now you have:\n",
        "    # - All pages from all PDFs in `all_docs`\n",
        "    # - Each entry has source filename + exact page number\n",
        "\n",
        "  return all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jC1-IRMtTRb"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Document Chunking\n",
        "#=======================\n",
        "def get_doc_chunked(doc_loaded: list = []) -> list:\n",
        "  chunks = []\n",
        "\n",
        "  if len(doc_loaded) > 0:\n",
        "    # Split with markdown-aware rules\n",
        "    splitter = MarkdownTextSplitter(\n",
        "        chunk_size=CONFIG[\"chunk_size\"],\n",
        "        chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
        "    )\n",
        "    chunks = splitter.split_documents(doc_loaded)\n",
        "\n",
        "    print(f\"Metadata example: {chunks[0].metadata}\")\n",
        "    # Should output: {\"source\": \"paper1.pdf\", \"page\": 1, \"total_pages\": 10}\n",
        "\n",
        "    print(f\"Chunk example: {chunks[0].page_content}\")\n",
        "    # Should show structured text like \"## Introduction\\n...\"\n",
        "\n",
        "    # Now you have chunks like:\n",
        "    # [\n",
        "    #   Document(\n",
        "    #       page_content=\"## Introduction\\n...\",\n",
        "    #       metadata={\"source\": \"paper1.pdf\", \"page\": 1}\n",
        "    #   ),\n",
        "    #   Document(\n",
        "    #       page_content=\"| Tool | Accuracy |\\n|------|----------|...\",\n",
        "    #       metadata={\"source\": \"paper2.pdf\", \"page\": 3}\n",
        "    #   )\n",
        "    # ]\n",
        "\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkdVa9XOubnL"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Document Embedding\n",
        "#=======================\n",
        "def get_retriever(doc_chunked: list = []):\n",
        "  embeddings = HuggingFaceEmbeddings(\n",
        "      model_name=CONFIG[\"embedding_model_name\"],\n",
        "      model_kwargs=CONFIG[\"embedding_model_kwargs\"],\n",
        "      encode_kwargs=CONFIG[\"embedding_encode_kwargs\"],\n",
        "      show_progress=True\n",
        "  )\n",
        "\n",
        "  if len(doc_chunked) > 0:\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=doc_chunked,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=CONFIG[\"persist_chroma_db\"]\n",
        "    )\n",
        "    print(f\"Vector store saved to {CONFIG['persist_chroma_db']}\")\n",
        "  else:\n",
        "    vector_store = Chroma(\n",
        "        persist_directory=CONFIG[\"persist_chroma_db\"],\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    print(f\"Vector store loaded from {CONFIG['persist_chroma_db']}\")\n",
        "\n",
        "  return vector_store.as_retriever(search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmRD-TtFqy00"
      },
      "outputs": [],
      "source": [
        "# Get the retriever\n",
        "def prepare_retriever():\n",
        "  if chroma_db_exists(CONFIG[\"persist_chroma_db\"]):\n",
        "    return get_retriever()\n",
        "\n",
        "  doc_sources = get_doc_sources()\n",
        "  doc_loaded = get_doc_loaded(doc_sources)\n",
        "  doc_chunked = get_doc_chunked(doc_loaded)\n",
        "  return get_retriever(doc_chunked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUkEFfpgKi_9"
      },
      "source": [
        "## Firebase Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en0tgWkJKqb8"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Firebase Utils\n",
        "#=======================\n",
        "\n",
        "# Load conversation history from Firestore\n",
        "def load_chat_history(\n",
        "    db: \"firestore.Client\",\n",
        "    collection: str,\n",
        "    user_id: str\n",
        ") -> List[BaseMessage]:\n",
        "  \"\"\"Load chat history from Firestore subcollection\"\"\"\n",
        "  messages = []\n",
        "\n",
        "  # Reference to the history subcollection\n",
        "  history_ref = db.collection(collection).document(user_id).collection(\"history\")\n",
        "\n",
        "  # Query all documents in the subcollection, ordered by timestamp\n",
        "  docs = history_ref.order_by(\"timestamp\").stream()\n",
        "\n",
        "  for doc in docs:\n",
        "    data = doc.to_dict()\n",
        "    if \"human\" in data:\n",
        "      messages.append(HumanMessage(content=data[\"human\"]))\n",
        "    if \"ai\" in data:\n",
        "      messages.append(AIMessage(content=data[\"ai\"]))\n",
        "\n",
        "  return messages\n",
        "\n",
        "# Save conversation to Firestore\n",
        "def save_chat_history(\n",
        "    db: \"firestore.Client\",\n",
        "    collection: str, user_id: str,\n",
        "    human_msg: str, ai_msg: str\n",
        "):\n",
        "  doc_ref = db.collection(collection).document(user_id)\n",
        "  message_ref = doc_ref.collection(\"history\").document()\n",
        "\n",
        "  message_ref.set({\n",
        "    \"human\": human_msg,\n",
        "    \"ai\": ai_msg,\n",
        "    \"timestamp\": firestore.SERVER_TIMESTAMP\n",
        "  })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8c8TMj6MBIy"
      },
      "source": [
        "## CORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o15v3ZEN1wPi"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# LLM\n",
        "#=======================\n",
        "def create_llm():\n",
        "  # os.environ['HUGGINGFACEHUB_API_KEY'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "  # llm = HuggingFaceEndpoint(\n",
        "  #     repo_id=CONFIG[\"llm_repo_id\"],\n",
        "  #     task=\"text-generation\",\n",
        "  #     huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_KEY'],\n",
        "  # )\n",
        "  # ===============================================================================\n",
        "  # hf = HuggingFacePipeline.from_model_id(\n",
        "  #     model_id=CONFIG[\"llm_repo_id\"],\n",
        "  #     task=\"text-generation\"\n",
        "  # )\n",
        "  # ===============================================================================\n",
        "  tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"llm_repo_id\"])\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      CONFIG[\"llm_repo_id\"],\n",
        "      torch_dtype=\"auto\",\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=32768\n",
        "  )\n",
        "  hf = HuggingFacePipeline(pipeline=pipe)\n",
        "  # ===============================================================================\n",
        "  chat = ChatHuggingFace(llm=hf)\n",
        "  return chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRZ84FHnUfa8"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# RAG Chain\n",
        "#=======================\n",
        "def rag_pipeline(retriever):\n",
        "  # Prompt template\n",
        "  system_template = \"\"\"You are a thesis tutor in Artificial Intelligence or Machine Learning.\n",
        "  Use the context to answer questions.\n",
        "  IMPORTANT!\n",
        "  1. Keep the context metadata with 'title' and 'page' fields along with your answer.\n",
        "  2. Count the sources used.\n",
        "  3. Number the sources used, e.g. the first context metadata become number 1, the second is number 2, and so on.\n",
        "  3. If you don't know the answer, just say that you don't know.\n",
        "\n",
        "  Context with sources:\n",
        "  {context}\n",
        "\n",
        "  Therefore, structure responses as:\n",
        "  ---------------------------------------------------------------------\n",
        "  Answer:\n",
        "  YOUR ANSWER HERE\n",
        "\n",
        "  Source:\n",
        "  source_number. title, page\n",
        "  source_number. title, page\n",
        "  etc.\n",
        "  ---------------------------------------------------------------------\"\"\"\n",
        "\n",
        "  # Create prompt\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", system_template),\n",
        "      (\"human\", \"{input}\")\n",
        "  ])\n",
        "\n",
        "  # Create LLM\n",
        "  llm = create_llm()\n",
        "\n",
        "  def format_docs_with_metadata(docs):\n",
        "    formatted = []\n",
        "    for i, doc in enumerate(docs):\n",
        "      metadata = doc.metadata\n",
        "      title = metadata.get(\"title\", \"Unknown title\")\n",
        "      page = metadata.get(\"page\", \"N/A\")\n",
        "      formatted.append(\n",
        "          f\"Document {i+1} (Title: {title}, Page: {page}):\\n{doc.page_content}\"\n",
        "      )\n",
        "    return \"\\n\\n\".join(formatted)\n",
        "\n",
        "  # Create chain using LCEL\n",
        "  rag_chain = (\n",
        "      {\n",
        "          \"context\": lambda x: format_docs_with_metadata(retriever.invoke(x[\"input\"])),\n",
        "          \"input\": lambda x: x[\"input\"]\n",
        "      }\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  return rag_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPhJ4HWyy9Im"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHLM5Mwxzs5v"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Equations\n",
        "#=======================\n",
        "# Process LaTeX equations from research papers.\n",
        "# Example input: '$\\frac{d}{dx}f(x)$'\n",
        "@tool\n",
        "def display_equation(latex_equation: str) -> str:\n",
        "  \"\"\"Convert a LaTeX equation into plainâ€‘text and symbolic form.\"\"\"\n",
        "  try:\n",
        "      # Convert LaTeX to plain text for calculation\n",
        "      plain_eq = LatexNodes2Text().latex_to_text(latex_equation).strip('$')\n",
        "      expr = sympify(plain_eq)\n",
        "      return latex(expr)\n",
        "  except Exception as e:\n",
        "      return f\"Could not process equation: {str(e)}\"\n",
        "\n",
        "#=======================\n",
        "# Code Generator\n",
        "#=======================\n",
        "# Generate code snippets\n",
        "@tool\n",
        "def generate_code(task: str) -> str:\n",
        "  \"\"\"Generate Python codes if needed based on specific task.\n",
        "  e.g. 1DCNN code, XGBoost code, etc.\"\"\"\n",
        "  client = TavilyClient(api_key=\"YOUR_TAVILY_API_KEY\")\n",
        "  results = client.search(\n",
        "      query=f\"Python code examples in {task}\",\n",
        "      max_results=5\n",
        "  )\n",
        "\n",
        "  prompt = \"\"\"You are a coder expert in Artificial Intelligence or Machine Learning.\n",
        "  YOU MUST write Python code examples for {task} using the context.\n",
        "  IMPORTANT!\n",
        "  1. Keep the context sources like 'title' and 'url' fields along with your answer.\n",
        "  2. Count the sources used.\n",
        "  3. Number the sources used, e.g. the first context metadata become number 1, the second is number 2, and so on.\n",
        "  3. If you don't know the answer, just say that you don't know.\n",
        "  Context:\n",
        "  {context}\"\"\"\n",
        "\n",
        "  llm = create_llm()\n",
        "  generate_code_chain = (\n",
        "      {\n",
        "          \"context\": lambda x: x[\"context\"],\n",
        "          \"task\": lambda x: x[\"task\"]\n",
        "      }\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "  return generate_code_chain.invoke({ \"task\": task, \"context\": results.to_string() })\n",
        "\n",
        "#=======================\n",
        "# Format Bibliography\n",
        "#=======================\n",
        "# Generate code snippets\n",
        "class BibEntry(TypedDict):\n",
        "  source_number: int\n",
        "  paper_title:   str\n",
        "  page_number:   int\n",
        "@tool\n",
        "def make_bibliography(entries: List[BibEntry]) -> str:\n",
        "    \"\"\"\n",
        "    Make bibliography based on multiple paper title and page number of the paper.\n",
        "    The paper title and page number is from the thesis_expert tool.\n",
        "    The source number is only for the numbering of the bibliography.\n",
        "\n",
        "    Args:\n",
        "      entries: a list of dicts, each with keys:\n",
        "        - source_number (int)\n",
        "        - paper_title (str)\n",
        "        - page_number (int)\n",
        "\n",
        "    Returns:\n",
        "      a multi-line bibliography string.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for item in entries:\n",
        "        num   = item[\"source_number\"]\n",
        "        title = item[\"paper_title\"]\n",
        "        page  = item[\"page_number\"]\n",
        "        lines.append(f\"{num}. {title}, page: {page}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "#=======================\n",
        "# RAG\n",
        "#=======================\n",
        "@tool\n",
        "def thesis_expert(user_question: str) -> str:\n",
        "  \"\"\"\n",
        "  Need to answer the question based on research paper.\n",
        "  \"\"\"\n",
        "  return rag_chain.invoke({\"input\": user_question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qi0kZAMwHwp"
      },
      "source": [
        "## History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIQ2CaVUwJf8"
      },
      "outputs": [],
      "source": [
        "# #=======================\n",
        "# # Firebase History\n",
        "# #=======================\n",
        "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
        "  \"\"\"In memory implementation of chat message history.\"\"\"\n",
        "  messages: list[BaseMessage] = Field(default_factory=list)\n",
        "\n",
        "  def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "    \"\"\"Add a list of messages to the store\"\"\"\n",
        "    self.messages.extend(messages)\n",
        "\n",
        "  def clear(self) -> None:\n",
        "    self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we use a global variable to store the chat message history.\n",
        "# This will make it easier to inspect it to see the underlying results.\n",
        "store = {}\n",
        "\n",
        "def get_session_history(\n",
        "    session_id: str,\n",
        "    firestore_client: \"firestore.Client\",\n",
        "    firebase_collection: str,\n",
        "    firebase_user_id: str\n",
        ") -> BaseChatMessageHistory:\n",
        "  print(f\"\"\"Getting session history for:\n",
        "  Session ID: {session_id}\n",
        "  Firestore Client: {firestore_client}\n",
        "  Firebase Collection: {firebase_collection}\n",
        "  Firebase User ID: {firebase_user_id}\n",
        "  =============================================================\n",
        "  \"\"\")\n",
        "  if session_id not in store:\n",
        "      store[session_id] = InMemoryHistory()\n",
        "\n",
        "      # Load existing history from Firestore\n",
        "      firestore_messages = load_chat_history(\n",
        "          firestore_client,\n",
        "          firebase_collection,\n",
        "          firebase_user_id\n",
        "      )\n",
        "\n",
        "      # Add loaded messages to the in-memory store\n",
        "      store[session_id].add_messages(firestore_messages)\n",
        "  return store[session_id]"
      ],
      "metadata": {
        "id": "f7Dr-jHDRXLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parser"
      ],
      "metadata": {
        "id": "8XGWrRecuOZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StrictThesisOutputParser(AgentOutputParser):\n",
        "  def parse(self, text: str) -> AgentFinish:\n",
        "    if \"Final Answer:\" not in text:\n",
        "      raise ValueError(f\"Missing 'Final Answer:' in response:\\\\n{text}\")\n",
        "    final_answer = text.split(\"Final Answer:\")[-1]\n",
        "    return AgentFinish(\n",
        "        return_values={\"output\": final_answer},\n",
        "        log=text\n",
        "    )"
      ],
      "metadata": {
        "id": "LgjyMTikuP5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c77csnnWbAyo"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1rXJMGp0SY7"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Agent\n",
        "#=======================\n",
        "def create_agent(rag_chain, retriever, db):\n",
        "  # Utilities needed\n",
        "  tools = [display_equation, generate_code, make_bibliography, thesis_expert]\n",
        "  llm = create_llm()\n",
        "\n",
        "  agent_prompt = \"\"\"You are a Thesis Tutor Expert in AI/ML. Follow STRICT steps:\n",
        "  Step 1: You MUST call thesis_expert on the user's question and get the core answer.\n",
        "  Step 2: Use any of the other tools except thesis_expert to support or beautify that answer.\n",
        "\n",
        "  Tools available:\n",
        "  {tools}\n",
        "\n",
        "  Follow this ReACT format:\n",
        "  Question: {input}\n",
        "  Thought: I will first gather the core answer using thesis_expert, KEEP the source and page metadata.\n",
        "  Action: thesis_expert\n",
        "  Action Input: {input}\n",
        "  Observation: <result from thesis_expert>\n",
        "\n",
        "  Thought: now I have the thesis-based answer. Do I need to generate code/equations/bibliography?\n",
        "  Action: the action to take, should be one of [{tool_names}], but EXCLUDING the thesis_expert\n",
        "  Action Input: think what you need to beautify\n",
        "  Observation: <result>\n",
        "  ...(you can repeat Thought/Action/Action Input/Observation as needed)\n",
        "\n",
        "  Thought: I now have all the information needed to answer. I need to return the final answer that ALWAYS starts with 'Final Answer:' and NEVER use JSON/markdown/URLs.\n",
        "  Final Answer: <comprehensive answer with any citations or references>\n",
        "\n",
        "  Begin!\n",
        "  Question: {input}\n",
        "  Thought: {agent_scratchpad}\"\"\"\n",
        "  agent_prompt = PromptTemplate.from_template(agent_prompt)\n",
        "\n",
        "  # Create the agent and executor\n",
        "  agent = create_react_agent(\n",
        "      llm=llm,\n",
        "      tools=tools,\n",
        "      prompt=agent_prompt,\n",
        "      stop_sequence=True,\n",
        "      output_parser=StrictThesisOutputParser()\n",
        "  )\n",
        "  agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "      agent=agent,\n",
        "      tools=tools,\n",
        "      handle_parsing_errors=True,\n",
        "      verbose=True\n",
        "  )\n",
        "\n",
        "  # Add memory usage into the agent executor\n",
        "  agent_with_memory = RunnableWithMessageHistory(\n",
        "      agent_executor,\n",
        "      get_session_history=get_session_history,\n",
        "      input_messages_key=\"input\",\n",
        "      history_messages_key=\"history\",\n",
        "      output_messages_key=\"output\",\n",
        "      history_factory_config=[\n",
        "          ConfigurableFieldSpec(\n",
        "              id=\"session_id\",\n",
        "              annotation=str,\n",
        "              name=\"Session ID\",\n",
        "              description=\"Unique identifier for the user session.\",\n",
        "              default=\"\",\n",
        "              is_shared=True,\n",
        "          ),\n",
        "          ConfigurableFieldSpec(\n",
        "              id=\"firestore_client\",\n",
        "              annotation=\"firestore.Client\",\n",
        "              name=\"Firestore Client\",\n",
        "              description=\"Firestore client to connect the class with Firebase Firestore.\",\n",
        "              default=None,\n",
        "              is_shared=True,\n",
        "          ),\n",
        "          ConfigurableFieldSpec(\n",
        "              id=\"firebase_collection\",\n",
        "              annotation=str,\n",
        "              name=\"Firebase Collection\",\n",
        "              description=\"Collection name in Firebase to load the chat from.\",\n",
        "              default=\"\",\n",
        "              is_shared=True,\n",
        "          ),\n",
        "          ConfigurableFieldSpec(\n",
        "              id=\"firebase_user_id\",\n",
        "              annotation=str,\n",
        "              name=\"Firebase User ID \",\n",
        "              description=\"Used to select data from specific ID in a Firebase collection.\",\n",
        "              default=\"\",\n",
        "              is_shared=True,\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  return agent_with_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq4YmMkfSokK"
      },
      "source": [
        "## Guardrails & Safety"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfZGOauyXm59"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Safety Utilities\n",
        "#=======================\n",
        "# Remove PII from user input\n",
        "def sanitize_input(\n",
        "    analyzer,\n",
        "    anonymizer,\n",
        "    text: str\n",
        ") -> str:\n",
        "  \"\"\"\n",
        "  Sanitize user input by removing PII.\n",
        "\n",
        "  Args:\n",
        "    analyzer: The Presidio analyzer engine\n",
        "    anonymizer: The Presidio anonymizer engine\n",
        "    text: The input text to sanitize\n",
        "\n",
        "  Returns:\n",
        "    Sanitized text with PII removed\n",
        "  \"\"\"\n",
        "  if not text or not text.strip():\n",
        "    return \"\"\n",
        "\n",
        "  # Analyze to find entities\n",
        "  results = analyzer.analyze(\n",
        "      text=text,\n",
        "      language=\"en\",\n",
        "      entities=CONFIG[\"safety_pii_entities\"]\n",
        "  )\n",
        "\n",
        "  # Anonymize the identified entities\n",
        "  anonymized_result = anonymizer.anonymize(\n",
        "      text=text,  # Make sure to pass the text parameter correctly\n",
        "      analyzer_results=results  # Pass the analyzer results\n",
        "  )\n",
        "\n",
        "  return anonymized_result.text\n",
        "\n",
        "# Fixed is_safe_input function\n",
        "def is_safe_input(\n",
        "    classifier,\n",
        "    text: str\n",
        ") -> bool:\n",
        "  \"\"\"\n",
        "  Check if input is safe based on classifier results.\n",
        "\n",
        "  Args:\n",
        "    classifier: The classification pipeline\n",
        "    text: The input text to check\n",
        "\n",
        "  Returns:\n",
        "    Boolean indicating if the input is safe\n",
        "  \"\"\"\n",
        "  if not text or not text.strip():\n",
        "    return True\n",
        "\n",
        "  try:\n",
        "    result = classifier(text)[0]\n",
        "    print(f\"Safety check score: {result['score']}\")\n",
        "    return result['score'] >= CONFIG[\"safety_threshold\"]  # Return TRUE if it's safe\n",
        "  except Exception as e:\n",
        "    print(f\"Safety check error: {e}\")\n",
        "    return False  # Fail closed - if error, assume unsafe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YBUNeUYcT7R"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2bjzpYEqX6-"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  #=======================\n",
        "  # Reset DB\n",
        "  #=======================\n",
        "  if CONFIG[\"RESET_DB\"]:\n",
        "    shutil.rmtree(CONFIG[\"persist_chroma_db\"], ignore_errors=True)\n",
        "\n",
        "  #=======================\n",
        "  # Firebase Setup\n",
        "  #=======================\n",
        "  cred = credentials.Certificate(CONFIG[\"firebase_key_file\"])\n",
        "  # Check if Firebase app is already initialized\n",
        "  try:\n",
        "    firebase_admin.get_app()\n",
        "  except ValueError:\n",
        "    # Firebase isn't initialized, so initialize it\n",
        "    firebase_admin.initialize_app(cred)\n",
        "  db = firestore.client()\n",
        "\n",
        "  #=======================\n",
        "  # Preparation\n",
        "  #=======================\n",
        "  retriever = prepare_retriever()\n",
        "  rag_chain = rag_pipeline(retriever)\n",
        "  agent_with_memory = create_agent(rag_chain, retriever, db)\n",
        "\n",
        "  #=======================\n",
        "  # Guardrails & Safety\n",
        "  #=======================\n",
        "  analyzer = AnalyzerEngine()\n",
        "  anonymizer = AnonymizerEngine()\n",
        "  safe_classifier = pipeline(\"text-classification\", model=CONFIG[\"safety_model_classifier\"])\n",
        "\n",
        "  print(\n",
        "      \"Welcome to Thesis Tutor!\\n\"\n",
        "      \"I could provide you informations about:\\n\"\n",
        "      \"\\t1. Explainable AI: SHAP.\\n\"\n",
        "      \"\\t2. XGBoost.\\n\"\n",
        "      \"\\t3. 1DCNN.\\n\"\n",
        "      \"\\t4. Usage of XAI and both model in finance.\\n\"\n",
        "      \"Type 'exit' to end.\"\n",
        "  )\n",
        "\n",
        "  # user_input = \"\"\"Tell me the usage of XGBoost in credit scoring.\n",
        "  # Also, tell me its strength instead of other models like CNN or Logistic Regression!\"\"\"\n",
        "\n",
        "  # #=======================\n",
        "  # # Conversation\n",
        "  # #=======================\n",
        "  # try:\n",
        "  #   # Input sanitization\n",
        "  #   clean_input = sanitize_input(analyzer, anonymizer, user_input)\n",
        "\n",
        "  #   # Content moderation\n",
        "  #   if not is_safe_input(safe_classifier, clean_input):\n",
        "  #     return print(\"I cannot respond to that request\")\n",
        "\n",
        "  #   # Context monitoring\n",
        "  #   if any(kw in clean_input for kw in CONFIG[\"safety_risk_keywords\"]):\n",
        "  #     return print(\"Security alert: Sensitive topic detected\")\n",
        "\n",
        "  #   # Invoke with context and history\n",
        "  #   response = agent_with_memory.invoke(\n",
        "  #       { \"input\": clean_input },\n",
        "  #       config={\n",
        "  #           \"configurable\": {\n",
        "  #               \"session_id\": CONFIG[\"user_session_id\"],\n",
        "  #               \"firestore_client\": db,\n",
        "  #               \"firebase_collection\": CONFIG[\"firebase_collection\"],\n",
        "  #               \"firebase_user_id\": CONFIG[\"firebase_user_id\"]\n",
        "  #           }\n",
        "  #       }\n",
        "  #   )\n",
        "  #   response = response[\"output\"]\n",
        "\n",
        "  #   # Output Validation\n",
        "  #   if not is_safe_input(safe_classifier, response):\n",
        "  #     response = \"I cannot provide that information\"\n",
        "\n",
        "  #   return print(f\"AI Response:\\n{response}\")\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"Your question: \").strip()\n",
        "    if user_input.lower() in ['exit']:\n",
        "      break\n",
        "\n",
        "    try:\n",
        "      # Input sanitization\n",
        "      clean_input = sanitize_input(analyzer, anonymizer, user_input)\n",
        "\n",
        "      # Content moderation\n",
        "      if not is_safe_input(safe_classifier, clean_input):\n",
        "        return print(\"I cannot respond to that request\")\n",
        "\n",
        "      # Context monitoring\n",
        "      if any(kw in clean_input for kw in CONFIG[\"safety_risk_keywords\"]):\n",
        "        return print(\"Security alert: Sensitive topic detected\")\n",
        "\n",
        "      # Invoke with context and history\n",
        "      response = agent_with_memory.invoke(\n",
        "          { \"input\": clean_input },\n",
        "          config={\n",
        "              \"configurable\": {\n",
        "                  \"session_id\": CONFIG[\"user_session_id\"],\n",
        "                  \"firestore_client\": db,\n",
        "                  \"firebase_collection\": CONFIG[\"firebase_collection\"],\n",
        "                  \"firebase_user_id\": CONFIG[\"firebase_user_id\"]\n",
        "              }\n",
        "          }\n",
        "      )\n",
        "      ai_output = response[\"output\"]\n",
        "\n",
        "      # Output Validation\n",
        "      if not is_safe_input(safe_classifier, ai_output):\n",
        "        response = \"I cannot provide that information\"\n",
        "\n",
        "      # Add chat history into the firebase\n",
        "      try:\n",
        "        save_chat_history(\n",
        "            db,\n",
        "            CONFIG[\"firebase_collection\"],\n",
        "            CONFIG[\"firebase_user_id\"],\n",
        "            user_input,\n",
        "            ai_output\n",
        "        )\n",
        "      except Exception as e:\n",
        "        print(f\"Error saving history: {e}\")\n",
        "\n",
        "      print(f\"AI Response:\\n{response}\")\n",
        "    except Exception as e:\n",
        "      print(f\"\\n:rotating_light: Error: {e}\")\n",
        "\n",
        "  print(\"\\nThank you for using the thesis tutor!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTGgigsaqNf8"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1P7zWfXB29QMsABNIx37qFkmrJvQCfQvK",
      "authorship_tag": "ABX9TyPle+/2Qx4CsEiQJPd0+nB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}